---
title: Assignment 4
author: "Melissa Wong"
date: \today
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r libraries, results='hide', message=FALSE, warning=FALSE}

library(tidyverse)
library(RColorBrewer)
library(rethinking)
```

```{r options}
options("scipen" = 1, "digits" = 4)

#knitr::opts_chunk$set(echo = FALSE)
#knitr::opts_chunk$set(message = FALSE)
#knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(out.width = "50%")
knitr::opts_chunk$set(fig.align = "center")

# Set Default ggplot palette
options(ggplot2.discrete.color=brewer.pal(8, "Dark2"))
options(ggplot2.discrete.fill=brewer.pal(8, "Dark2"))
```

# Problem 1

```{r}
birds <- tibble(speciesA = c(0.2, 0.8, 0.05),
                speciesB = c(0.2, 0.1, 0.15),
                speciesC = c(0.2, 0.05, 0.7),
                speciesD = c(0.2, 0.025, 0.05),
                speciesE = c(0.2, 0.025, 0.05))

# Compute entropy
apply(birds, 1, function(x) -sum(x * log(x)))
```

Island 1 has the greatest entropy; this is consistent with the fact that all species are equally probable which is the least surprising distribution is absence of any prior information.

On the other hand, Island 2 has the least entropy, which is consistent with one species (A in this case) being far more likely than the other species.

Island 3's entropy is slightly larger than Island 2 because while one species (C) is more likely than the others, the probabilities are slightly more diffuse than Island 2.

```{r}

for (row in 1:3)
{
  print(paste("target = Island", row))
  p <- birds[row,]
  q <- birds[-row,]
  print(apply(q, 1, function(x) sum(p * log(p) - p * log(x))))
}
```

The KL divergence is smallest (best prediction) when the target = Island 3 and the model = Island 1.

The KL divergence is largest (worst prediction) when target = Island 2 and model = Island 3.  

# Problem 2

```{r}
df <- sim_happiness(seed=1377, N_years=1000) %>% 
  filter(age > 17) %>% 
  mutate(A = (age - 18)/(65-18),
         mid = married + 1)
summary(df)

m6.9 <- quap(
  alist(
    happiness ~ dnorm(mu, sigma),
    mu <- a[mid] + bA * A,
    a[mid] ~ dnorm(0, 1),
    bA ~ dnorm(0, 2),
    sigma ~ dexp(1)
  ), data=df
)
precis(m6.9, depth=2)

m6.10 <- quap(
  alist(
    happiness ~ dnorm(mu, sigma),
    mu <- a + bA * A,
    a ~ dnorm(0, 1),
    bA ~ dnorm(0, 2),
    sigma ~ dexp(1)
  ), data=df
)
precis(m6.10, depth=2)
```

```{r}
compare(m6.9, m6.10)
```

# Problem 3

```{r}
data(foxes)

df <- foxes %>%
  mutate(across(avgfood:weight, scale))

mdl1 <- quap(
  alist(
    weight ~ dnorm(mu_weight, sigma_weight),
    mu_weight <- a + b * avgfood + c * groupsize + d * area,
    a ~ dnorm(0, 1),
    b ~ dnorm(0, 1),
    c ~ dnorm(0, 1),
    d ~ dnorm(0, 1),
    sigma_weight ~ dexp(1)
  ),
  data = df
)

mdl2 <- quap(
  alist(
    weight ~ dnorm(mu_weight, sigma_weight),
    mu_weight <- a + b * avgfood + c * groupsize,
    a ~ dnorm(0, 1),
    b ~ dnorm(0, 1),
    c ~ dnorm(0, 1),
    sigma_weight ~ dexp(1)
  ),
  data = df
)

mdl3 <- quap(
  alist(
    # area -> weight
    weight ~ dnorm(mu_weight, sigma_weight),
    mu_weight <- a + b * groupsize + c * area,
    a ~ dnorm(0, 1),
    b ~ dnorm(0, 1),
    c ~ dnorm(0, 1),
    sigma_weight ~ dexp(1)
  ),
  data = df
)

mdl4 <- quap(
  alist(
    # area -> weight
    weight ~ dnorm(mu_weight, sigma_weight),
    mu_weight <- a + b * avgfood,
    a ~ dnorm(0, 1),
    b ~ dnorm(0, 1),
    sigma_weight ~ dexp(1)
  ),
  data = df
)

mdl5 <- quap(
  alist(
    weight ~ dnorm(mu_weight, sigma_weight),
    mu_weight <- a + b * area,
    a ~ dnorm(0, 1),
    b ~ dnorm(0, 1),
    sigma_weight ~ dexp(1)
  ),
  data = df
)

compare(mdl1, mdl2, mdl3, mdl4, mdl5)
compare(mdl1, mdl2, mdl3, mdl4, mdl5)@dSE

plot(compare(mdl1, mdl2, mdl3, mdl4, mdl5))
```

Unsurprisingly, mdl1 which includes all three predictors (_area_, _groupsize_, _avgfood_) has the smallest WAIC and thus highest predictive accuracy.

The _dSE_ is much larger than the difference in WAIC for mdl1, mdl2 and mdl3. We can interpret this as saying once we know _groupsize_ and one of the other two predictors, adding the third predictor doesn't give us much additional information.  Similary, there is very little difference between mdl4 and mdl5--once we know either _area_ or _avgfood_, the other predictor doesn't add much information.  This is consistent with the DAG where causal effect of _area_ has a forward path entirely through _avgfood_ with no backdoor paths.


