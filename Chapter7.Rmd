---
title: Chapter 7
author: "Melissa Wong"
date: \today
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r libraries, results='hide', message=FALSE, warning=FALSE}

library(tidyverse)
library(RColorBrewer)
library(rethinking)
```

```{r options}
options("scipen" = 1, "digits" = 4)

#knitr::opts_chunk$set(echo = FALSE)
#knitr::opts_chunk$set(message = FALSE)
#knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(out.width = "50%")
knitr::opts_chunk$set(fig.align = "center")

# Set Default ggplot palette
options(ggplot2.discrete.color=brewer.pal(8, "Dark2"))
options(ggplot2.discrete.fill=brewer.pal(8, "Dark2"))
```


# 7E1

1. The measure of uncertainty should be continuous.

2. The measure of uncertainty should increase as the number of possible events increases.

3. The measure of uncertainty should be additive.

# 7E2

```{r}
p_coin <- c(0.5, 0.5)
(entropy <- -sum(p_coin * log(p_coin)))
```

# 7E3

```{r}
p_die <- c(0.2, 0.25, 0.25, 0.3)
(entropy <- -sum(p_die * log(p_die)))
```

# 7E4

```{r}
p_die <- c(1/3, 1/3, 1/3)
(entropy <- -sum(p_die * log(p_die)))
```

# 7M1

AIC =  -2 * log pointwise predictive density + 2 * number of free parameters

WAIC = -2 * log pointwise predictive density - 2 * penalty term where the penalty term is is the sum of the variances of the log-probabilities for each observation.

WAIC is more general as it doesn't make any distributional assumptions about the posterior.

AIC is a reliable approximation to the cross-validation deviance when

1. The priors are flat or overwhelmed by the data.

2. The posterior distribution is approximately multivariate Gaussian.

3. The sample size is much greater than the number of parameters.

# 7M2

_Model selection_ means choosing the model with the lowest criterion value and discarding the other models.  This effectively discards information about the differences in relative model accuracy. Also, this approach maximizes predictive accuracy which isn't necessarily useful if we're interested in causation.

_Model comparison_ uses multiple models and the differences in accuracy between them to understand how different variables effect the model and to help infer causal relationships (conditional on the models considered).

# 7M3

The information criteria estimate the difference in log probability (or divergence) between the target and the model. Comparing models effectively takes the difference of those differences to find which model is relatively closer to the target. However, the true target log probability is unknown, so for that relative difference to be meaningful both models must have the same reference point (i.e., both models must be fit to the same observations).

# 7M4

```{r}
# Simulate data
N <- 50
x <- seq(-1, 1, length.out = N)
y <- 2 + 2 * x + rnorm(N, 0, 0.3)
df <- data.frame(x = x, y = y)

res <- data.frame(prior_sd = c(0.5, 1, 3, 5),
                  WAIC = rep(NA, 4),
                  PSIS = rep(NA, 4))

for (idx in seq_along(res$prior_sd))
{
  mdl <- quap(
    alist(
      y ~ dnorm(mu, sigma),
      mu <- a + b * x,
      a ~ dnorm(0, res$prior_sd[idx]),
      b ~ dnorm(0, res$prior_sd[idx]),
      sigma ~ dexp(1)
    ),
    data = df
  )
  
  res$WAIC[idx] <- WAIC(mdl)$penalty
  res$PSIS[idx] <- PSIS(mdl)$penalty
}

knitr::kable(res)
```

The penalty term increases as the prior becomes less informative.  This is because the data has more influence and thus the overfitting risk increases with the less informative prior.

# 7M5

See 7M4 explanation.

# 7M6

See 7M4 explanation.

# 7H1

```{r}
data(Laffer)

df <- Laffer %>% mutate_if(is.numeric, scale)
df %>% ggplot() + geom_point(mapping=aes(x=tax_rate, y=tax_revenue))
```

```{r}
# Fit linear model

mdl_lin <- quap(
  alist(
    tax_revenue ~ dnorm(mu, sigma),
    mu <- a + b * tax_rate,
    a ~ dnorm(0, 1),
    b ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = df
)

data.frame(tax_rate = df$tax_rate,
                  ypred = t(link(mdl_lin, n=10))) %>% pivot_longer(cols=-tax_rate, names_to="iter",
                     values_to="ypred") %>%
  ggplot() +
  geom_line(mapping=aes(x=tax_rate, y=ypred, 
                        group=iter), alpha=0.2) +
  geom_point(data=df, mapping=aes(x=tax_rate, y=tax_revenue)) +
  labs(title="Linear Model", x="tax_rate", y="tax_revenue") 
  
```

```{r}
# Fit quadratic

df$tax_rate2 <- df$tax_rate^2

mdl_quad <- quap(
  alist(
    tax_revenue ~ dnorm(mu, sigma),
    mu <- a + b1 * tax_rate + b2 * tax_rate2,
    a ~ dnorm(0, 1),
    b1 ~ dnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = df
)

data.frame(tax_rate = df$tax_rate,
                  ypred = t(link(mdl_quad, n=10))) %>% pivot_longer(cols=-tax_rate, names_to="iter",
                     values_to="ypred") %>%
  ggplot() +
  geom_line(mapping=aes(x=tax_rate, y=ypred, 
                        group=iter), alpha=0.2) +
  geom_point(data=df, mapping=aes(x=tax_rate, y=tax_revenue)) +
  labs(title="Linear Model", x="tax_rate", y="tax_revenue") 
  
```

```{r}
compare(mdl_lin, mdl_quad, func=WAIC)
compare(mdl_lin, mdl_quad, func=PSIS)
```

The standard error for the difference is >> than the difference for both WAIC and PSIS.  Thus we really can't distinguish between the two models (i.e., adding the quadratic term does not significanty improve the fit).

# 7H2

```{r}
# Linear model
data.frame(waic = WAIC(mdl_lin, pointwise = TRUE)$penalty,
           psis = PSIS(mdl_lin, pointwise = TRUE)$k) %>%
  ggplot() +
  geom_point(mapping=aes(x=psis, y=waic))
```
```{r}
# Quadratic Model

data.frame(waic = WAIC(mdl_quad, pointwise = TRUE)$penalty,
           psis = PSIS(mdl_quad, pointwise = TRUE)$k) %>%
  ggplot() +
  geom_point(mapping=aes(x=psis, y=waic))
```

Refit models with student-t distribution

```{r}
mdl_lin_t <- quap(
  alist(
    tax_revenue ~ dstudent(2, mu, sigma),
    mu <- a + b * tax_rate,
    a ~ dnorm(0, 1),
    b ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = df
)

data.frame(tax_rate = df$tax_rate,
                  ypred = t(link(mdl_lin_t, n=10))) %>% pivot_longer(cols=-tax_rate, names_to="iter",
                     values_to="ypred") %>%
  ggplot() +
  geom_line(mapping=aes(x=tax_rate, y=ypred, 
                        group=iter), alpha=0.2) +
  geom_point(data=df, mapping=aes(x=tax_rate, y=tax_revenue)) +
  labs(title="Linear Model", x="tax_rate", y="tax_revenue") 
  
data.frame(waic = WAIC(mdl_lin_t, pointwise = TRUE)$penalty,
           psis = PSIS(mdl_lin_t, pointwise = TRUE)$k) %>%
  ggplot() +
  geom_point(mapping=aes(x=psis, y=waic))
```

```{r}
mdl_quad_t <- quap(
  alist(
    tax_revenue ~ dstudent(2, mu, sigma),
    mu <- a + b1 * tax_rate + b2 * tax_rate2,
    a ~ dnorm(0, 1),
    b1 ~ dnorm(0, 1),
    b2 ~ dnorm(0, 1),
    sigma ~ dexp(1)
  ),
  data = df
)

data.frame(tax_rate = df$tax_rate,
                  ypred = t(link(mdl_quad_t, n=10))) %>% pivot_longer(cols=-tax_rate, names_to="iter",
                     values_to="ypred") %>%
  ggplot() +
  geom_line(mapping=aes(x=tax_rate, y=ypred, 
                        group=iter), alpha=0.2) +
  geom_point(data=df, mapping=aes(x=tax_rate, y=tax_revenue)) +
  labs(title="Linear Model", x="tax_rate", y="tax_revenue") 
  
data.frame(waic = WAIC(mdl_quad_t, pointwise = TRUE)$penalty,
           psis = PSIS(mdl_quad_t, pointwise = TRUE)$k) %>%
  ggplot() +
  geom_point(mapping=aes(x=psis, y=waic))
```
# 7H3

# 7H4

# 7H5

