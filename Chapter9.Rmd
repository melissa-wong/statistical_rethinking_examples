---
title: Chapter 9
author: "Melissa Wong"
date: \today
output:
  html_document:
    df_print: paged
  pdf_document: default
---

```{r libraries, results='hide', message=FALSE, warning=FALSE}

library(tidyverse)
library(RColorBrewer)
library(rethinking)
library(brms)
```

```{r options}
options("scipen" = 1, "digits" = 4)

#knitr::opts_chunk$set(echo = FALSE)
#knitr::opts_chunk$set(message = FALSE)
#knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(out.width = "50%")
knitr::opts_chunk$set(fig.align = "center")

# Set Default ggplot palette
options(ggplot2.discrete.color=brewer.pal(8, "Dark2"))
options(ggplot2.discrete.fill=brewer.pal(8, "Dark2"))
```

# 9E1

3. The simple Metropolis algorithm requires that the proposal distribution must be symmetric.

# 9E2

Gibbs sampling requires that the conditional distribution of each variable is known, and samples are drawn in succession from each conditional distribution.  This is more efficient than Metropolis sampling when it can be used, but is limited to problems with conjugate priors.

# 9E3 

HMC doesn't inherently work for discrete parameters because it needs a continuous surface upon which to simulate the path of the particle (which determines where to sample). HMC requires some modifications to work with discrete parameters.

# 9E4

Typically the samples in a Markov Chain are correlated.  So _n_eff_ is equivalent to the number of independent samples.

# 9E5

_Rhat_ should approach 1 when the chain is sampling correctly from the posterior.

# 9E6

The three characteristics of a "healthy" trace plot are:

1. Good mixing - each chain rapidly explores the parameter space rather than getting "stuck" near one value and changing slowly.

2. Convergence - the chains converge to approximately the same value.

3. Stationarity - the mean of each chain is relatively stable.

# 9E7

The characteristics of a "healthy" trace rank plot are:

1. The ranks for each chain should be approximately uniform.

2. The chains should mostly overlap.

# 9M1

```{r}
data(rugged)

df <- rugged %>%
  drop_na(rgdppc_2000) %>%
  transmute(log_gdp = log(rgdppc_2000),
         rugged_std = rugged / max(rugged),
         cid = ifelse(cont_africa==1, "Africa", "Not_Africa")) %>%
  mutate(log_gdp_std = log_gdp / mean(log_gdp))

# Define formula
# Below is an MLM which is not correct
# f <- bf(log_gdp_std ~ 1|cid + rugged_std|cid)
# See https://bookdown.org/content/4857/markov-chain-monte-carlo.html#easy-hmc-ulam-brm
f <- bf(
  log_gdp_std ~ 0 + a + b * (rugged_std - 0.215),
  a ~ 0 + cid,
  b ~ 0 + cid,
  nl=TRUE
)

# Check default priors
get_prior(f, data=df)

m9M1 <- brm(formula=f, data=df,
            prior = c(
              # Note: use coef="cidAfrica"/"cidNot_Africa" if want to specify
              # different priors for each intercept
              # Same prior for both intercepts
              set_prior("normal(0, 0.1)", class="b", nlpar="a"),
              # Same prior for both slopes
              set_prior("normal(0, 0.3)", class="b", nlpar="b"),
              #set_prior("exponential(1)", class="sigma")
              set_prior("uniform(0,1)", class="sigma")
            ),
            file="m9M1", file_refit = "on_change",
            chains=1, cores=1)

summary(m9M1)
```
```{r}

df_ulam <- df %>%
  select(log_gdp_std, rugged_std, cid) %>%
  mutate(cid = ifelse(cid=="Africa", 1, 2))

m9.1 <- ulam(
  alist(
    log_gdp_std ~ dnorm(mu, sigma),
    mu <- a[cid] + b[cid]*(rugged_std - 0.215),
    a[cid] ~ dnorm(1, 0.1),
    b[cid] ~ dnorm(0, 0.3),
    sigma ~ dunif(0,1)
  ),
  data=df_ulam, chains=1, cores=1
)

precis(m9.1, depth=2)
```

```{r}
# Extract the stan code from the brms model
stancode(m9M1)
# How does it compare to the stan code from the ulam model?
cat(m9.1@model)
```

# 9M2


```{r}

m9M2 <- brm(formula=f, data=df,
            prior = c(
              # Note: use coef="cidAfrica"/"cidNot_Africa" if want to specify
              # different priors for each intercept
              # Same prior for both intercepts
              set_prior("normal(0, 0.1)", class="b", nlpar="a"),
              # Same prior for both slopes
              # set_prior("normal(0, 0.3)", class="b", nlpar="b"),
              set_prior("exponential(0.3)", class="b", nlpar="b", lb=0),
              set_prior("exponential(1)", class="sigma")
            ),
            file="m9M2", file_refit = "on_change",
            chains=1, cores=1)

summary(m9M2)
```

```{r}
stancode(m9M2)
```


```{r}

df_ulam <- df %>%
  select(log_gdp_std, rugged_std, cid) %>%
  mutate(cid = ifelse(cid=="Africa", 1, 2))

m9.1 <- ulam(
  alist(
    log_gdp_std ~ dnorm(mu, sigma),
    mu <- a[cid] + b[cid]*(rugged_std - 0.215),
    a[cid] ~ dnorm(1, 0.1),
    b[cid] ~ dexp(0.3),
    sigma ~ dexp(1)
  ),
  data=df_ulam, chains=1, cores=1
)

precis(m9.1, depth=2)
```

# 9H1

```{r}
mp <- ulam(
  alist(
    a ~ dnorm(0,1),
    b ~ dcauchy(0,1)
  ), data=list(y=1), chains=1
)

precis(mp)

traceplot(mp)

pairs(mp@stanfit)
```
This model is simply attempting to fit the Normal and Cauchy distributions. The mean and the variance for the Cauchy distribution are both undefined, which means that the parameters space for _b_ is completely unrestricted.  The _pairs_ plot illustrates how the MCMC samples are pretty evenly distributed over the posterior distribution for _a_; however the samples are mostly concentrated around the center of _b_'s distribution and very sparsely in the tails.  There just isn't enough information to get a reasonable estimate of _b_'s posterior distribution.

# 9H2

```{r}
data("WaffleDivorce")

df <- WaffleDivorce %>%
  transmute(D = scale(Divorce),
         M = scale(Marriage),
         A = scale(MedianAgeMarriage))

```

## `rethinking`

```{r}
m5.1 <- ulam(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bA * A,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data=df,
  log_lik = TRUE,
  chains=1, cores=1
)

m5.2 <- ulam(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bM * M,
    a ~ dnorm(0, 0.2),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data=df,
  log_lik = TRUE,
  chains=1, cores=1
)

m5.3 <- ulam(
  alist(
    D ~ dnorm(mu, sigma),
    mu <- a + bA * A + bM * M,
    a ~ dnorm(0, 0.2),
    bA ~ dnorm(0, 0.5),
    bM ~ dnorm(0, 0.5),
    sigma ~ dexp(1)
  ),
  data=df,
  log_lik = TRUE,
  chains=1, cores=1
)
```

```{r}
compare(m5.1, m5.2, m5.3)

compare(m5.1, m5.2, m5.3, func=PSIS)
```

## `brms`

```{r}

m5.1 <- brm(
  formula = D ~ A,
  data = df,
  prior = c(
    set_prior("normal(0, 0.2)", class="Intercept"),
    set_prior("normal(0, 0.5)", class="b"),
    set_prior("exponential(1)", class="sigma")
  ),
  file="m9H2a", file_refit = "on_change",
  chains=1, cores=1
)
# Add WAIC and PSIS to model
m5.1 <- add_criterion(m5.1, criterion = c("loo", "waic"))

m5.2 <- brm(
  formula = D ~ M,
  data = df,
  prior = c(
    set_prior("normal(0, 0.2)", class="Intercept"),
    set_prior("normal(0, 0.5)", class="b")
  ),
  file="m9H2b", file_refit = "on_change",
  chains=1, cores=1
)
# Add WAIC and PSIS to model
m5.2 <- add_criterion(m5.2, criterion = c("loo", "waic"))

m5.3 <- brm(
  formula = D ~ M + A,
  data = df,
  prior = c(
    set_prior("normal(0, 0.2)", class="Intercept"),
    set_prior("normal(0, 0.5)", class="b")
  ),
  file="m9H2c", file_refit = "on_change",
  chains=1, cores=1
)
# Add WAIC and PSIS to model
m5.3 <- add_criterion(m5.3, criterion = c("loo", "waic"))
```

```{r}
# Compare using WAIC
loo_compare(m5.1, m5.2, m5.3, criterion = "waic") %>% 
  print(simplify=FALSE) %>% 
  knitr::kable()

# Compare using PSIS
loo_compare(m5.1, m5.2, m5.3, criterion = "loo") %>% 
  print(simplify=FALSE) %>%
  knitr::kable()
```

# 9H3

```{r}
N <- 100
set.seed(909)
height <- rnorm(N, 10, 2)
leg_prop <- runif(N, 0.4, 0.5)
left_leg <- leg_prop * height + rnorm(N, 0, 0.02)
right_leg <- leg_prop * height + rnorm(N, 0, 0.02)

d <- data.frame(height, right_leg, left_leg)

m5.8s <- ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + bl * left_leg + br * right_leg,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dexp(1)
  ),
  data=d, chains=4,
  start=list(a=10, bl=0, br=0.1, sigma=1),
  log_lik = TRUE
)

m5.8s2 <-ulam(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + bl * left_leg + br * right_leg,
    a ~ dnorm(10, 100),
    bl ~ dnorm(2, 10),
    br ~ dnorm(2, 10),
    sigma ~ dexp(1)
  ),
  data=d, chains=4,
  constraints=list(br="lower=0"),
  start=list(a=10, bl=0, br=0.1, sigma=1),
  log_lik = TRUE
)
```


```{r}
plot(precis(m5.8s))

plot(precis(m5.8s2))
```

Once `br` is constrained to be positive, then the coefficients of `m5.8s2` tell us that `bl` doesn't give any significant info about `height`.  

# 9H4

```{r}
compare(m5.8s, m5.8s2, func=WAIC)
```
The two models are effectively the same with regards to predictive accuracy. `m5.8s2` has fewer effective parameters (`pWAIC=2.8`); this can be attributed to the more informative prior resulting in a model less likely to overfit.





