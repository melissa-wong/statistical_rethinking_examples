---
title: "Chapter 3"
author: "Melissa Wong"
date: \today
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r results='hide', message=FALSE, warning=FALSE}
options("scipen" = 1, "digits" = 4)

#knitr::opts_chunk$set(echo = FALSE)
# knitr::opts_chunk$set(message = FALSE)
# knitr::opts_chunk$set(warning = FALSE)
# knitr::opts_chunk$set(out.width = "50%")
# knitr::opts_chunk$set(fig.align = "center")
library(tidyverse)

library(rethinking)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```


# Examples

## Sampling from grid approximate posterior

```{r}
N <- 1000
p_grid <- seq(0, 1, length.out=N)
prob_p <- rep(1, N)
prob_data <- dbinom(6, size=9, prob=p_grid)
posterior <- prob_data * prob_p
posterior <- posterior / sum(posterior)

samples <- sample(p_grid, prob=posterior, size=1e4, replace=TRUE)
plot(samples)
```
```{r}
dens(samples)
```

## Sammpling to summarize

```{r}
# Probability posterior < 0.5
sum(posterior[p_grid < 0.5])
sum(samples < 0.5) / 1e4
```

```{r}
# 80% Credible interval - Percentile
quantile(samples, c(0.1, 0.9))
```

```{r}
# Credible interval for highly skewed distribution
N <- 1000
p_grid <- seq(0, 1, length.out=N)
prob_p <- rep(1, N)
prob_data <- dbinom(3, size=3, prob=p_grid)
posterior <- prob_data * prob_p
posterior <- posterior / sum(posterior)

samples <- sample(p_grid, prob=posterior, size=1e4, replace=TRUE)
CI <- PI(samples, 0.5)
hpdi <- HPDI(samples, 0.5)

plot(p_grid, posterior, type='l')
abline(v=CI[1], col="blue")
abline(v=CI[2], col="blue")
abline(v=hpdi[1], col="red")
abline(v=hpdi[2], col="red")
```

```{r}
# Maximum a posteriori estimate
p_grid[which.max(posterior)]
# Mean
mean(samples)
# Median
median(samples)
```

```{r}
# Choose absolute value loss function d - p
# Choose d = 0.5
sum(posterior*abs(0.5-p_grid))

# Find optimal p_hat
loss <- sapply(p_grid, function(d) sum(posterior*abs(d-p_grid)))
p_grid[which.min(loss)]
```

The median optimizes this loss function.

Two most common loss functions are

1. $|d-p|$ which leads to posterior median

2. $(d-p)^2$ which leads to posterior mean

## Posterior Predictive Distribution

This is how we propogate the model uncertainty as we evaluate the implied predictions.  We do this by averaging over the posterior density for $p$.


```{r}
N <- 1000
p_grid <- seq(0, 1, length.out=N)
prob_p <- rep(1, N)
prob_data <- dbinom(6, size=9, prob=p_grid)
posterior <- prob_data * prob_p
posterior <- posterior / sum(posterior)

samples <- sample(p_grid, prob=posterior, size=1e4, replace=TRUE)
```

```{r}
# predictions with a single value from posterior
w <- rbinom(1e4, size=9, prob=0.6)
simplehist(w)
```

```{r}
# Posterior predictive distribution
w <- rbinom(1e4, size=9, prob=samples)
simplehist(w)
```

## Other model checks

Our sequence was W L W W W L W L W, so we count the number of switches (6) and the longest run length (3).  Now calculate those same statistics from samples.

# Practice Problems

## Easy Problems

```{r}
N = 1000
p_grid <- seq(0, 1, length.out=N)
prior <- rep(1, N)
likelihood <- dbinom(6,9, prob=p_grid)
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
set.seed(100)
samples <- sample(p_grid, prob=posterior, size=1e4, replace=TRUE)
```


```{r}
# 3E1
sum(samples < 0.2) / 1e4
```

```{r}
# 3E2

sum(samples > 0.8) / 1e4
```

```{r}
# 3E3
sum(0.2 <= samples & samples <= 0.8) / 1e4
```

```{r}
# 3E4
quantile(samples, 0.2)
```

```{r}
# 3E5
quantile(samples, 0.8)
```

```{r}
# 3E6
HPDI(samples, 0.66)
```

```{r}
# 3E7
quantile(samples, c(0.17, 0.73))
```

## Medium Problems

```{r}
# 3M1

N = 1000
p_grid <- seq(0, 1, length.out=N)
prior <- rep(1, N)
likelihood <- dbinom(8,15, prob=p_grid)
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
plot(p_grid, posterior, type="l")
```

```{r}
# 3M2
set.seed(100)
samples <- sample(p_grid, prob=posterior, size=1e4, replace=TRUE)
HPDI(samples, 0.9)
```

```{r}
# 3M3
ppd <- rbinom(1e4, size=15, prob=samples)
simplehist(ppd)
sum(ppd == 8)/1e4
```

```{r}
# 3M4
sum(ppd == 6) / 1e4
```

```{r}
#3M5
N = 1000
p_grid <- seq(0, 1, length.out=N)
prior <- ifelse(p_grid < 0.5, 0, 1)
likelihood <- dbinom(8,15, prob=p_grid)
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
plot(p_grid, posterior, type="l")

set.seed(100)
samples <- sample(p_grid, prob=posterior, size=1e4, replace=TRUE)
HPDI(samples, 0.9)

ppd <- rbinom(1e4, size=15, prob=samples)
simplehist(ppd)
sum(ppd == 8)/1e4

sum(ppd == 6) / 1e4
```

The better prior means greater weight is given to values above 0.5.  The 90% HPDI is narrower and the probability that d=8 is higher.

```{r}
# 3M6
N = 1000
p_grid <- seq(0, 1, length.out=N)
prior <- ifelse(p_grid < 0.5, 0, 1)
likelihood <- dbinom(800,1500, prob=p_grid)
posterior <- likelihood * prior
posterior <- posterior / sum(posterior)
plot(p_grid, posterior, type="l")

posterior_samples <- sample(p_grid, prob=posterior, size=1e4, replace=TRUE)
quantile(posterior_samples, c(0.005, 0.995))
```

## Hard problems

```{r}
data(homeworkch3)
boys <- sum(birth1) + sum(birth2)
girls <- length(birth1) + length(birth2) - boys
```

```{r}
# 3H1

N <- 1000
p_grid <- seq(0, 1, length.out=N)
likelihood <- dbinom(boys, boys+girls, prob=p_grid)
post.unstd <- likelihood * p_grid
post <- post.unstd/sum(post.unstd)
plot(p_grid, post, type="l", xlab="Pr[boy]", ylab="density")
p_grid[which.max(post)]
```

```{r}
# 3H2
samples <- sample(p_grid, 1e4, replace=TRUE, prob=post)
HPDI(samples, prob=c(0.5, 0.89, 0.97))

```
```{r}
# 3H3

# Posterior predictions
post_pred <- rbinom(1e4, 200, prob=samples)
dens(post_pred)
```

The most likely outcome is around 110 boys which is consistent with the observed data of 111 boys.

```{r}
# 3H4
# Posterior predictions
post_pred <- rbinom(1e4, 100, prob=samples)
dens(post_pred)
abline(v=sum(birth1))
HPDI(post_pred, prob=c(0.5, 0.89, 0.97))
```

The observed value of 51 births is no longer the most likely value of the posterior predictive distribution, but is is still well within the 89% and 97% HPDIs. So this isn't a terrible model, but it could be improved.

```{r}
# 3H5

# Number of first births that are girls
girls1 <- sum(birth1 == 0)
# Number of boys following girl
boys2 <- sum(birth2[birth1 == 0])

# Posterior predictions
post_pred <- rbinom(1e4, girls1, prob=samples)
dens(post_pred)
abline(v=sum(boys2))
HPDI(post_pred, prob=c(0.5, 0.89, 0.97))
```

The observed number of boys (39) is much higher than the most likely value (27) of the posterior predictive distribution.  So the independence assumption does not appear to be valid.  But why?


```{r}
# Same analysis but now look at # girls following 1st boy

# Number of first births that are boys
boys1 <- sum(birth1 == 1)
# Number of girls following boy
girls2 <- sum(birth2[birth1 == 1])

# Posterior predictions
post_pred <- rbinom(1e4, boys1, prob=samples)
dens(post_pred)
abline(v=sum(girls2))
HPDI(post_pred, prob=c(0.5, 0.89, 0.97))
```

The observed number of girls (21) is much lower than the most likely value (27) of the posterior predictive distribution.  